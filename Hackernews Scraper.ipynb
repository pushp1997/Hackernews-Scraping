{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hackernews Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries\n",
    "\n",
    "* Requests: To make http(s) requests\n",
    "* BeautifulSoup4: To \n",
    "* PyMongo: To interact with mongodb database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from pymongo import MongoClient\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Extracting the MetaData of all posts from all pages specified by the User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a function that returns the parsed html soup of a link given as parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_to_soup(link):\n",
    "    response = requests.get(link)\n",
    "    if response.ok:\n",
    "        return BeautifulSoup(response.text, 'html.parser')\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requesting to get the home page of TheHackerNews.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_page_soup = link_to_soup('https://thehackernews.com/')\n",
    "#print(home_page_soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to parse the html document as well. Let's ask the user, for how many pages, the data has to be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "No_pages = int(input('How many pages to extract the data from? '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will parse the the rest of the pages and append all of the pages to the array 'pages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "pages.append(home_page_soup)\n",
    "No_pages -= 1\n",
    "for i in range(No_pages):\n",
    "    next_page_link = pages[i].find(\"a\", class_=\"blog-pager-older-link-mobile\")['href']\n",
    "    pages.append(link_to_soup(next_page_link))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the pages parsed to extract the data from. So, let's extract all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_url_title_data = []\n",
    "posts_url_others_data = []\n",
    "\n",
    "for page in pages:\n",
    "    posts_in_page = home_page_soup.find_all(\"a\", class_='story-link')\n",
    "    for post in posts_in_page:\n",
    "        posts_url_title_data.append({\n",
    "        \"url\" : post['href'],\n",
    "        \"title\" : post.find(\"h2\", class_='home-title').text\n",
    "        })\n",
    "        \n",
    "        posts_url_others_data.append({\n",
    "        \"url\" : post['href'],\n",
    "        \"desc\" : post.find(\"div\", class_='home-desc').text,\n",
    "        \"author\" : post.find(\"span\").text[1:len(post.find(\"span\").text)-1],\n",
    "        \"imgSrc\" : post.find(\"img\")['data-src']\n",
    "        })\n",
    "    \n",
    "print('URL of 1st post: ', posts_url_title_data[0]['url'])\n",
    "print('Description of 1st post: ', posts_url_others_data[0]['desc'])\n",
    "print('Author of the 1st post: ', posts_url_others_data[0]['author'])\n",
    "print('Image source of the 1st post: ', posts_url_others_data[0]['imgSrc'])\n",
    "print('Title of the 1st post: ', posts_url_title_data[0]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata of all posts from all pages is extracted. Phase 1 completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2a:  Store the data in a MongoDb database\n",
    "\n",
    "We'll store the data in two documents having:\n",
    "1. Url and title of the blog\n",
    "2. Url and Desription, Image, Author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Let's connect to the mongodb server by asking the user MongoDbURI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mongoDbURI = input('Enter the mongoDbURI: ')\n",
    "client = MongoClient(mongoDbURI) # for users running mongo server on local machine: 'mongodb://localhost:27017/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the database\n",
    "db = client['hackernews']\n",
    "\n",
    "# Getting the collections\n",
    "urlTitle = db['url-title']\n",
    "urlOthers = db['url-others']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connection done, now we'll insert the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = urlTitle.insert_many(posts_url_title_data)\n",
    "result2 = urlOthers.insert_many(posts_url_others_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the values were inserted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result1.inserted_ids[:5])\n",
    "print(result2.inserted_ids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the Object Ids of the inserted documents, that means the documents were inserted to the database successfully.\n",
    "\n",
    "Phase 2a completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2b: Save the data as JSON\n",
    "\n",
    "We will save the data as a single JSON file, merging `posts_url_title_data` with `posts_url_others_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data into a single dictionary\n",
    "n = len(posts_url_title_data)\n",
    "posts_url_data = [ dict(posts_url_title_data[0], **posts_url_others_data[0]) for i in range(n) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_location = input(\"Enter the location to save the JSON to: \")\n",
    "output_path = Path(output_location)\n",
    "\n",
    "# Create any necessary parent directories\n",
    "if not output_path.parent.exists():\n",
    "    print(\"Directory {} does not exist, creating.\".format(output_path.parent))\n",
    "    output_path.parent.mkdir(parents=True)\n",
    "\n",
    "# Save JSON\n",
    "with open(output_path, \"w\") as output_file:\n",
    "    print(\"Saving to {}.\".format(output_path))\n",
    "    output_file.write(json.dumps(posts_url_data))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2c: Save data in MySQL using pymysql ORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## connect to mysql db\n",
    "import pymysql\n",
    "def mysqlconnect():\n",
    "    # To connect MySQL database\n",
    "    conn = pymysql.connect(\n",
    "        host='localhost',\n",
    "        user='root', \n",
    "        password = \"password\",\n",
    "        db='scraper',\n",
    "        )\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn_obj = mysqlconnect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cursor = conn_obj.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL relation model setup queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB creation\n",
    "sql_db_create = \"create database if not exists scraper;\"\n",
    "\n",
    "# Use DB created\n",
    "sql_use_db = \"use scraper;\"\n",
    "\n",
    "# create table \"urlTitle\" to store [urlLink, urlTitle]\n",
    "sql_create_urlTitle_table = \"create table if not exists urltitle(\\\n",
    "  url_title_id int auto_increment primary key,\\\n",
    "  url_link text not null,\\\n",
    "  url_title text not null)\"\n",
    "\n",
    "# create table \"urlotherInfo\" to store [urlLink, urlTitle, urlAuthor, urlImgSrc]\n",
    "sql_create_urlOtherInfo_table = \"create table if not exists urlotherinfo(\\\n",
    "  id int auto_increment primary key,\\\n",
    "  url_link text not null,\\\n",
    "  url_description text not null,\\\n",
    "  author text not null,\\\n",
    "  imgsrc text not null)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Queries execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cursor.execute(sql_db_create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cursor.execute(\"use scraper\")\n",
    "output=conn_cursor.fetchall()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cursor.execute(sql_create_urlTitle_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cursor.execute(sql_create_urlOtherInfo_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cursor.execute(\"show tables\")\n",
    "output = conn_cursor.fetchall()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the data in DB tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_insert_urlTitle = \"insert into urltitle(url_link, url_title) values (%s, %s)\"\n",
    "sql_insert_urlOtherInfo = \"insert into urlotherinfo(url_link, url_description, author, imgSrc) values (%s, %s, %s, %s)\"\n",
    "for url_title, url_other in zip(posts_url_title_data, posts_url_others_data):\n",
    "    conn_cursor.execute(sql_insert_urlTitle, (url_title[\"url\"], url_title[\"title\"]))\n",
    "    output = conn_cursor.fetchall()\n",
    "    conn_cursor.execute(sql_insert_urlOtherInfo, (url_other[\"url\"], url_other[\"desc\"], url_other[\"author\"], url_other[\"imgSrc\"]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cursor.execute(\"select * from urltitle\")\n",
    "output=conn_cursor.fetchall()\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
